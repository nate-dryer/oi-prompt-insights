# Open Interpreter Prompt Insights Tool

A Streamlit app for analyzing and optimizing prompts used with Open Interpreter.

## Table of Contents
- [Quick Start](#quick-start)
- [About](#about)
- [Features](#key-features)
- [Usage](#usage)
- [Contributing](#contributing)

## Quick Start

```bash
git clone https://github.com/your-username/open-interpreter-prompt-insights.git
pip install -r requirements.txt
streamlit run app.py

## About

This tool helps users track, analyze, and improve prompts for [Open Interpreter](https://github.com/OpenInterpreter/open-interpreter). It offers an intuitive interface for recording prompt performance and visualizing analytics.

## Why This Tool?

Minor variations in prompt wording can significantly impact token usage and code execution. For instance, two similar prompts led to vastly different outcomes:

- Prompt A: Simple execution, efficient token use
- Prompt B: Complex local code execution, 100x more tokens used

This tool aims to:

1. Track prompt variations and outcomes
2. Analyze prompt efficiency
3. Identify effective patterns
4. Develop best practices
5. Enhance Open Interpreter user experience

## Key Features

- Record and categorize prompts
- Visualize prompt effectiveness
- Track satisfaction rates and execution times
- Compare token usage across prompts
- Interactive analytics dashboard
- Data persistence across sessions

## Usage

After launching the app:

1. Use the sidebar to navigate
2. Record new prompts
3. View analytics
4. Explore insights
5. Compare prompt variations

## Contributing

We welcome contributions! To get started:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

For more details, see [CONTRIBUTING.md](CONTRIBUTING.md).

---

For more on Open Interpreter, visit the [official repository](https://github.com/OpenInterpreter/open-interpreter) by [KillianLucas](https://github.com/KillianLucas).
